{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense 3D Face Correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:24.185898Z",
     "start_time": "2019-05-12T11:22:24.183258Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"12\" \n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"12\" \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"12\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:25.116159Z",
     "start_time": "2019-05-12T11:22:24.795911Z"
    }
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time, warnings\n",
    "import re\n",
    "import threading\n",
    "import cv2\n",
    "import ipyvolume as ipv\n",
    "import scipy\n",
    "from math import cos, sin\n",
    "from scipy import meshgrid, interpolate\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial import ConvexHull, Delaunay\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# THRESHOLDS\n",
    "rho = 0.5\n",
    "eigen_ratio_threshold = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Each face data, normalize it and then interpolate it parallely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:25.826879Z",
     "start_time": "2019-05-12T11:22:25.818608Z"
    }
   },
   "outputs": [],
   "source": [
    "#Read each face data, normalize it and get the interpolation it in a parallel fashion\n",
    "def get_data(file_path,var_name):\n",
    "    #global points and grid data structure which will be modified by all threads\n",
    "    global face_points\n",
    "    global grid_data\n",
    "    holder = []\n",
    "    #reading face data from path\n",
    "    with open(file_path, \"r\") as vrml:\n",
    "        for line in vrml:\n",
    "            a = line.strip().strip(\",\").split()\n",
    "            if len(a) == 3:\n",
    "                try:\n",
    "                    holder.append(list(map(float, a)))\n",
    "                except:\n",
    "                    pass\n",
    "    x,y,z = zip(*holder)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    z = np.array(z)\n",
    "    holder = np.array(holder)\n",
    "    #normalizing face\n",
    "    maxind = np.argmax(holder[:,2])\n",
    "    nosex = holder[maxind,0]\n",
    "    nosey = holder[maxind,1]\n",
    "    nosez = holder[maxind,2]\n",
    "    holder = holder - np.array([nosex, nosey, nosez])\n",
    "    face_points[var_name] = holder\n",
    "    # grid data extraction\n",
    "    x1, y1, z1 = map(np.array, zip(*holder))\n",
    "    grid_x, grid_y = np.mgrid[np.amin(x1):np.amax(x1):0.5, np.amin(y1):np.amax(y1):0.5]\n",
    "    grid_z = griddata((x1, y1), z1, (grid_x, grid_y), method='linear')\n",
    "    grid_data[var_name] = [grid_x, grid_y, grid_z]  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:26.439854Z",
     "start_time": "2019-05-12T11:22:26.239470Z"
    }
   },
   "outputs": [],
   "source": [
    "face_points = {}\n",
    "grid_data = {}\n",
    "file_paths = {\n",
    "    \"path1\": \"F0001/F0001_AN01WH_F3D.wrl\",\n",
    "    \"path2\": \"F0001/F0001_AN02WH_F3D.wrl\",\n",
    "    \"path3\": \"F0001/F0001_AN03WH_F3D.wrl\",\n",
    "    \"path4\": \"F0001/F0001_AN04WH_F3D.wrl\",\n",
    "    \"path5\": \"F0001/F0001_DI01WH_F3D.wrl\",\n",
    "    \"path6\": \"F0001/F0001_DI02WH_F3D.wrl\",\n",
    "    \"path7\": \"F0001/F0001_DI03WH_F3D.wrl\",\n",
    "    \"path8\": \"F0001/F0001_DI04WH_F3D.wrl\",\n",
    "    \n",
    "}\n",
    "\n",
    "for i in range(1,len(file_paths)+1):\n",
    "    thread = threading.Thread(target=get_data,args=(file_paths[\"path\"+str(i)],\"face\"+str(i)))\n",
    "    thread.start()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Correspondence Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed points sampling using mean 2D convex hull "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:31.932239Z",
     "start_time": "2019-05-12T11:22:27.739304Z"
    }
   },
   "outputs": [],
   "source": [
    "def hull72(points, nosex, nosey, nosez):\n",
    "    newhull = [[nosex, nosey, nosez]]\n",
    "    for theta in range(0, 360, 5):\n",
    "        fx = 200 * cos(theta * np.pi / 180)\n",
    "        fy = 200 * sin(theta * np.pi / 180)\n",
    "        nearest_point = min(zip(points[:, 0], points[:, 1], points[:, 2]), key=lambda p:(p[0] - fx)**2 + (p[1] - fy)**2)\n",
    "        newhull.append(nearest_point)\n",
    "    return newhull\n",
    "\n",
    "def get_hull(points):\n",
    "    maxind = np.argmax(points[:,2])\n",
    "    # coordinates of nose, nosex = x coordinate of nose, similarly for nosey and nosez\n",
    "    nosex = points[maxind,0]\n",
    "    nosey = points[maxind,1]\n",
    "    nosez = points[maxind,2]\n",
    "    hull = np.array(hull72(points, nosex,nosey,nosez))\n",
    "    return hull\n",
    "\n",
    "hull = np.zeros([73, 3])\n",
    "for i in range(1, len(file_paths)+1):\n",
    "    hull += get_hull(face_points[\"face\" + str(i)])\n",
    "hull = hull / len(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delaunay Triangulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:35.182837Z",
     "start_time": "2019-05-12T11:22:35.178531Z"
    }
   },
   "outputs": [],
   "source": [
    "def triangulation(hull):\n",
    "    points2D = np.vstack([hull[:,0],hull[:,1]]).T\n",
    "    tri_hull = Delaunay(points2D) \n",
    "    return tri_hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:35.610941Z",
     "start_time": "2019-05-12T11:22:35.607293Z"
    }
   },
   "outputs": [],
   "source": [
    "tri_hull = triangulation(hull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geodesic Patch Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:36.720586Z",
     "start_time": "2019-05-12T11:22:36.706610Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_all_patches_for_face(face_index, hull, triangles):\n",
    "    from itertools import combinations\n",
    "    points = face_points[\"face\"+str(face_index)]\n",
    "    patch_width = 5 * rho\n",
    "    def distance(x,y,z,x1,y1,z1,x2,y2,z2):\n",
    "        a = (y2-y1)/(x2-x1)\n",
    "        b = -1\n",
    "        c = y2-x2*(y2-y1)/(x2-x1)\n",
    "        return abs(a*x+b*y+c)/(a**2+b**2)**0.5\n",
    "    \n",
    "    all_patches = []\n",
    "    for t1,t2 in combinations(triangles,r=2): #pairwise triangles\n",
    "        if len(set(t1)&set(t2))==2:           #triangles with a common edge\n",
    "            patch_list = []\n",
    "            a_ind, b_ind = list(set(t1)&set(t2))\n",
    "            x1, y1, z1 = hull[a_ind,:]\n",
    "            x2, y2, z2 = hull[b_ind,:]\n",
    "            for x,y,z in points: #loop over all points to find patch points\n",
    "                if (x-x1/2-x2/2)**2+(y-y1/2-y2/2)**2<(x1/2-x2/2)**2+(y1/2-y2/2)**2 and distance(x,y,z,x1,y1,z1,x2,y2,z2)<patch_width:\n",
    "                    patch_list.append([x,y,z])\n",
    "            if len(patch_list)==0: \n",
    "                #print(\"ALERT: NO PATCH FOR AN EDGE!!!!\")\n",
    "                pass\n",
    "            all_patches.append(np.array(patch_list))\n",
    "    global patches\n",
    "    for edge_index in range(len(all_patches)):\n",
    "        patches[\"edge\" + str(edge_index)].append(all_patches[edge_index])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:37.395604Z",
     "start_time": "2019-05-12T11:22:37.392715Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_patches(hull, triangles):\n",
    "    threads = []\n",
    "    for face_index in range(1, len(file_paths)+1):\n",
    "        thread = threading.Thread(target=get_all_patches_for_face, args=(face_index, hull, triangles))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    for thread in threads: \n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:38.073425Z",
     "start_time": "2019-05-12T11:22:38.071444Z"
    }
   },
   "outputs": [],
   "source": [
    "# patches = defaultdict(list) # key = edges, values = a list of extracted patches from all faces along that edge \n",
    "# update_patches(hull, tri_hull.simplices)\n",
    "# '''Wait till `patches` is completely initialized '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:38.750913Z",
     "start_time": "2019-05-12T11:22:38.749067Z"
    }
   },
   "outputs": [],
   "source": [
    "# patches.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoint Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:40.516231Z",
     "start_time": "2019-05-12T11:22:40.510086Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes in a point and the patch it belongs to and decides whether it is a keypoint (ratio of largest two eigenvalues on the covariance matrix of its local surface) or not\n",
    "def is_keypoint(point, points):\n",
    "    threshold = 7 * rho\n",
    "    nhood = points[(np.sum(np.square(points-point),axis=1)) < threshold**2]\n",
    "    try:\n",
    "        nhood = (nhood - np.min(nhood, axis=0)) / (np.max(nhood, axis=0) - np.min(nhood, axis=0))\n",
    "        covmat = np.cov(nhood)\n",
    "        eigvals = np.sort(np.abs(np.linalg.eigvalsh(covmat)))\n",
    "        ratio = eigvals[-1]/(eigvals[-2]+0.0001)\n",
    "        return ratio>30 #eigen_ratio_threshold #/ 5\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def get_keypoints_from_patch(edge_index):\n",
    "    global keypoints\n",
    "    edge_patches = patches[\"edge\" + str(edge_index)]\n",
    "    edge_keypoints = []\n",
    "    for patch in edge_patches:\n",
    "        #print(patch.shape)\n",
    "        if patch.shape[0]:\n",
    "            patch_keypoints = patch[np.apply_along_axis(is_keypoint, 1, patch, patch)] # keypoints in `patch`\n",
    "        else:\n",
    "            patch_keypoints = []\n",
    "        edge_keypoints.append(patch_keypoints)\n",
    "    keypoints[\"edge\" + str(edge_index)] = edge_keypoints \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:41.387307Z",
     "start_time": "2019-05-12T11:22:41.384717Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_keypoints(patches):\n",
    "    threads = []\n",
    "    for edge_index in range(1, len(patches)+1):\n",
    "        thread = threading.Thread(target=get_keypoints_from_patch, args=(edge_index,))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    for thread in threads: \n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:42.211037Z",
     "start_time": "2019-05-12T11:22:42.206761Z"
    }
   },
   "outputs": [],
   "source": [
    "#keypoints = {} # key = edge, value = a list of keypoints extracted from the patches along that edge across all faces\n",
    "#update_keypoints(patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:42.768027Z",
     "start_time": "2019-05-12T11:22:42.763826Z"
    }
   },
   "outputs": [],
   "source": [
    "#keypoints.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:43.818641Z",
     "start_time": "2019-05-12T11:22:43.809691Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_normal(x, y, grid_x, grid_y, grid_z):\n",
    "    '''\n",
    "      3\n",
    "    1   2\n",
    "      4\n",
    "    x, y are coordinates of the point for which the normal has to be calculated\n",
    "    '''\n",
    "    i = (x - grid_x[0, 0]) / (grid_x[1, 0] - grid_x[0, 0])\n",
    "    j = (y - grid_y[0, 0]) / (grid_y[0, 1] - grid_y[0, 0])\n",
    "    i,j = int(round(i)), int(round(j))\n",
    "    if (not 0 <= i < grid_x.shape[0]-1) or (not 0 <= j < grid_y.shape[1]-1):\n",
    "        warnings.warn(\"out of bounds error\")\n",
    "        #pdb.set_trace()\n",
    "        return \"None\"\n",
    "    point1 = (grid_x[i-1, j], grid_y[i-1, j], grid_z[i-1, j])\n",
    "    point2 = (grid_x[i+1, j], grid_y[i+1, j], grid_z[i+1, j])\n",
    "    point3 = (grid_x[i, j-1], grid_y[i, j-1], grid_z[i, j-1])\n",
    "    point4 = (grid_x[i, j+1], grid_y[i, j+1], grid_z[i, j+1])\n",
    "    a1, a2, a3 = [point2[x] - point1[x] for x in range(3)]\n",
    "    b1, b2, b3 = [point3[x] - point4[x] for x in range(3)]\n",
    "    normal = np.array([a3*b2, a1*b3, -a1*b2])\n",
    "    return normal/np.linalg.norm(normal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:44.549605Z",
     "start_time": "2019-05-12T11:22:44.529571Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_keypoint_features(keypoints, face_index):\n",
    "    feature_list = [] # a list to store extracted features of each keypoint\n",
    "    final_keypoints = [] # remove unwanted keypoints, like the ones on edges etc\n",
    "    for point in keypoints:\n",
    "        point_features = []\n",
    "        x, y, z = point\n",
    "        points = face_points[\"face\" + str(face_index)]\n",
    "        grid_x, grid_y, grid_z = grid_data[\"face\" + str(face_index)]\n",
    "        threshold = 5 * rho\n",
    "        nhood = points[(np.sum(np.square(points-point), axis=1)) < threshold**2]\n",
    "        xy_hu_moments = cv2.HuMoments(cv2.moments(nhood[:, :2])).flatten()\n",
    "        yz_hu_moments = cv2.HuMoments(cv2.moments(nhood[:, 1:])).flatten()\n",
    "        xz_hu_moments = cv2.HuMoments(cv2.moments(nhood[:, ::2])).flatten()\n",
    "        hu_moments = np.concatenate([xy_hu_moments, yz_hu_moments, xz_hu_moments])\n",
    "        normal = get_normal(x, y, grid_x, grid_y, grid_z)\n",
    "        if normal == \"None\": # array comparision raises ambiguity error, so None passed as string\n",
    "            continue\n",
    "        final_keypoints.append(point)\n",
    "        point_features.extend(np.array([x, y, z])) # spatial location\n",
    "        point_features.extend(normal)\n",
    "        point_features.extend(hu_moments)\n",
    "        point_features = np.array(point_features)\n",
    "        \n",
    "        feature_list.append(point_features)\n",
    "    final_keypoints = np.array(final_keypoints)\n",
    "    return final_keypoints, feature_list\n",
    "\n",
    "def get_features(edge_index):\n",
    "    global features, keypoints \n",
    "    edgewise_keypoint_features = [] # store features of keypoints for a given edge_index across all faces\n",
    "    for face_index in range(1, len(file_paths)+1):\n",
    "        try:\n",
    "            edge_keypoints = keypoints[\"edge\" + str(edge_index)][face_index-1]\n",
    "            final_keypoints, keypoint_features = get_keypoint_features(edge_keypoints, face_index)\n",
    "            keypoints[\"edge\" + str(edge_index)][face_index-1] = final_keypoints # update the keypoint, remove unwanted keypoints like those on the edge etc\n",
    "        except: # for no keypoints, no features\n",
    "            keypoint_features = []\n",
    "        edgewise_keypoint_features.append(keypoint_features)\n",
    "    features[\"edge\" + str(edge_index)] = edgewise_keypoint_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:45.764853Z",
     "start_time": "2019-05-12T11:22:45.762203Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_features(keypoints):\n",
    "    threads = []\n",
    "    for edge_index in range(1, len(keypoints)+1):\n",
    "        thread = threading.Thread(target=get_features, args=(edge_index, ))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    for thread in threads: \n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:46.618753Z",
     "start_time": "2019-05-12T11:22:46.617083Z"
    }
   },
   "outputs": [],
   "source": [
    "#features = {} # key = edge + edge_index, value = list of features for each keypoint across all the faces\n",
    "#update_features(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:47.200802Z",
     "start_time": "2019-05-12T11:22:47.198793Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:47.934420Z",
     "start_time": "2019-05-12T11:22:47.923793Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_keypoint_under_2rho(keypoints, point):\n",
    "    \"\"\"return the index of the keypoint in `keypoints` which is closest to `point` if that distance is less than 2 * rho, else return None\"\"\"\n",
    "    try:\n",
    "        distance = np.sqrt(np.sum(np.square(keypoints-point), axis=1))\n",
    "        if (distance < 3*rho).any():\n",
    "            min_dist_index = np.argmin(distance)\n",
    "            return min_dist_index\n",
    "    except Exception as e: # keypoints is [], gotta return None\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def get_matching_keypoints(edge_keypoints, edge_features, edge_index):\n",
    "    # check if a bunch of keypoints across the patches (across all faces) are withing 2*rho and their euclidean dist < Kq\n",
    "    # first get all the keypoints in a list\n",
    "    matching_keypoints_list = []\n",
    "    for face_index1 in range(len(edge_keypoints)): # take a patch along the edge among the faces\n",
    "        for point_index, point in enumerate(edge_keypoints[face_index1]): # take a keypoint in that patch, we have to find corresponding keypoints in each other patche along this edge\n",
    "            matched_keypoint_indices = [] # to store indices of matched keypoints across the patches\n",
    "            for face_index2 in range(len(edge_keypoints)): # find if matching keypoints exist across the patches along that edge across all faces\n",
    "                if face_index2 == face_index1: \n",
    "                    matched_keypoint_indices.append(point_index)\n",
    "                    continue\n",
    "                matched_keypoint = get_keypoint_under_2rho(edge_keypoints[face_index2], point)\n",
    "                if matched_keypoint:\n",
    "                    #if edge_index == 36: pdb.set_trace()I#\n",
    "                    matched_keypoint_indices.append(matched_keypoint)\n",
    "                else: # no keypoint was matched in the above patch (face_index2), gotta start search on other keypoint from face_index1\n",
    "                    break\n",
    "                        \n",
    "            if len(matched_keypoint_indices) == len(edge_keypoints): # there's a corresponding keypoint for each patch across all faces\n",
    "                 matching_keypoints_list.append(matched_keypoint_indices)\n",
    "    if len(matching_keypoints_list) == 0:\n",
    "        return []\n",
    "    # now we have those keypoints which are in vicinity of 2*rho, let's compute euclidean distance of their feature vectors\n",
    "    Kq = 2\n",
    "    final_matched_keypoints = []\n",
    "    for matched_keypoints in matching_keypoints_list: # select first list of matching keypoints\n",
    "        # get the indices, get their corresponding features, compute euclidean distance\n",
    "        try:\n",
    "            features = np.array([edge_features[face_index][idx] for face_index, idx in zip(range(len(edge_features)), matched_keypoints)])\n",
    "            euc_dist_under_kq = lambda feature, features: np.sqrt(np.sum(np.square(features - feature), axis=1)) < Kq\n",
    "            if np.apply_along_axis(euc_dist_under_kq, 1, features, features).all() == True:\n",
    "                # we have got a set of matching keypoints, get their mean coordinates\n",
    "                matched_coords = [edge_keypoints[face_index][idx] for face_index, idx in zip(range(len(edge_features)), matched_keypoints)]\n",
    "                final_matched_keypoints.append(np.mean(matched_coords, axis=0))\n",
    "        except:\n",
    "            pdb.set_trace()\n",
    "    return final_matched_keypoints\n",
    "\n",
    "def keypoint_matching_thread(edge_index):\n",
    "    global new_keypoints, edge_keypoints, edge_features\n",
    "    edge_keypoints = keypoints[\"edge\" + str(edge_index)]\n",
    "    edge_features = features[\"edge\" + str(edge_index)]\n",
    "    matched_keypoints = get_matching_keypoints(edge_keypoints, edge_features, edge_index)\n",
    "    if len(matched_keypoints):\n",
    "        new_keypoints.extend(matched_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:49.337193Z",
     "start_time": "2019-05-12T11:22:49.332944Z"
    }
   },
   "outputs": [],
   "source": [
    "# those keypoints which are in vicinity of 2*rho are considered for matching\n",
    "# matching is done using constrained nearest neighbour\n",
    "# choose an edge, select a keypoint, find out keypoints on corresponding patches on other faces within a vicinity of 2*rho, \n",
    "# get euclidean distance in features among all possible pair wise combinations, if the distances come out to be less than Kp are added to the global set of correspondences\n",
    "def keypoint_matching(keypoints, features):\n",
    "    thread = []\n",
    "    for edge_index in range(1, len(keypoints)+1):\n",
    "        thread = threading.Thread(target=keypoint_matching_thread, args=(edge_index, ))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    for thread in threads: \n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:50.352013Z",
     "start_time": "2019-05-12T11:22:50.347774Z"
    }
   },
   "outputs": [],
   "source": [
    "#new_keypoints = []\n",
    "#keypoint_matching(keypoints, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:51.119527Z",
     "start_time": "2019-05-12T11:22:51.117601Z"
    }
   },
   "outputs": [],
   "source": [
    "#new_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:22:51.587583Z",
     "start_time": "2019-05-12T11:22:51.585103Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#keypoint_matching(keypoints, features)\n",
    "#new_keypoints = np.array(new_keypoints)\n",
    "#new_keypoints = np.unique(new_keypoints, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:23:02.631509Z",
     "start_time": "2019-05-12T11:23:02.125147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "face_points = {}\n",
    "grid_data = {}\n",
    "file_paths = {\n",
    "    \"path1\": \"F0001/F0001_AN01WH_F3D.wrl\",\n",
    "    \"path2\": \"F0001/F0001_AN02WH_F3D.wrl\",\n",
    "    \"path3\": \"F0001/F0001_AN03WH_F3D.wrl\",\n",
    "    \"path4\": \"F0001/F0001_AN04WH_F3D.wrl\",\n",
    "    \"path5\": \"F0001/F0001_DI01WH_F3D.wrl\",\n",
    "    \"path6\": \"F0001/F0001_DI02WH_F3D.wrl\",\n",
    "    \"path7\": \"F0001/F0001_DI03WH_F3D.wrl\",\n",
    "    \"path8\": \"F0001/F0001_DI04WH_F3D.wrl\",\n",
    "    \n",
    "}\n",
    "\n",
    "for i in range(1,len(file_paths)+1):\n",
    "    threads = []\n",
    "    thread = threading.Thread(target=get_data,args=(file_paths[\"path\"+str(i)],\"face\"+str(i)))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "for thread in threads: \n",
    "    thread.join()\n",
    "print(len(face_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:23:10.993177Z",
     "start_time": "2019-05-12T11:23:06.779545Z"
    }
   },
   "outputs": [],
   "source": [
    "hull = np.zeros([73, 3])\n",
    "for i in range(1, len(file_paths)+1):\n",
    "    hull += get_hull(face_points[\"face\" + str(i)])\n",
    "hull = hull / len(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T11:39:35.916924Z",
     "start_time": "2019-05-12T11:38:26.584751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting iteration:  0\n",
      "Starting Delaunay triangulation............Done | time taken: 0.0009 seconds\n",
      "Starting geodesic patch extraction............Done | time taken: 18.8962 seconds\n",
      "Starting keypoint extraction............"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ags/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done | time taken: 7.3148 seconds\n",
      "Starting feature extraction............"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ags/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  app.launch_new_instance()\n",
      "/home/ags/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: out of bounds error\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done | time taken: 2.1400 seconds\n",
      "Starting keypoint matching............Done | time taken: 0.2057 seconds\n",
      "Total new correspondences found:  5\n",
      "Updating correspondence set...\n",
      "Iteration completed in 28.5581 seconds\n",
      "\n",
      "\n",
      "Starting iteration:  1\n",
      "Starting Delaunay triangulation............Done | time taken: 0.0009 seconds\n",
      "Starting geodesic patch extraction............Done | time taken: 22.6260 seconds\n",
      "Starting keypoint extraction............Done | time taken: 15.3494 seconds\n",
      "Starting feature extraction............Done | time taken: 2.4728 seconds\n",
      "Starting keypoint matching............Done | time taken: 0.3119 seconds\n",
      "No new keypoints found\n"
     ]
    }
   ],
   "source": [
    "# Start correspondence densification loop\n",
    "num_iterations = 10\n",
    "correspondence_set = hull\n",
    "global patches, keypoints, features, new_keypoints\n",
    "\n",
    "patches = defaultdict(list) # key = edges, values = a list of extracted patches from all faces along that edge \n",
    "keypoints = {} # key = edge, value = a list of keypoints extracted from the patches along that edge across all faces\n",
    "features = {} # key = edge + edge_index, value = list of features for each keypoint across all the faces\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    new_keypoints = []\n",
    "    print(\"\\n\\nStarting iteration: \", iteration)\n",
    "    t1 = time.time()\n",
    "    print(\"Starting Delaunay triangulation............\", end=\"\", flush=True)\n",
    "    tri_hull = triangulation(correspondence_set)\n",
    "    print(\"Done | time taken: %0.4f seconds\" % (time.time() - t1))\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(\"Starting geodesic patch extraction............\", end=\"\", flush=True)\n",
    "    update_patches(correspondence_set, tri_hull.simplices)\n",
    "    print(\"Done | time taken: %0.4f seconds\" % (time.time() - t2))\n",
    "\n",
    "    t3 = time.time()\n",
    "    print(\"Starting keypoint extraction............\", end=\"\", flush=True)\n",
    "    update_keypoints(patches)\n",
    "    print(\"Done | time taken: %0.4f seconds\" % (time.time() - t3))\n",
    "\n",
    "    t4 = time.time()\n",
    "    print(\"Starting feature extraction............\", end=\"\", flush=True)\n",
    "    update_features(keypoints)\n",
    "    print(\"Done | time taken: %0.4f seconds\" % (time.time() - t4))\n",
    "\n",
    "    t5 = time.time()\n",
    "    print(\"Starting keypoint matching............\", end=\"\", flush=True)\n",
    "    \n",
    "    keypoint_matching(keypoints, features)\n",
    "    print(\"Done | time taken: %0.4f seconds\" % (time.time() - t5))\n",
    "    \n",
    "    if len(new_keypoints) == 0:\n",
    "        print(\"No new keypoints found\")\n",
    "        break\n",
    "    \n",
    "    new_keypoints = np.unique(np.array(new_keypoints), axis=0)\n",
    "    print(\"Total new correspondences found: \", len(new_keypoints))\n",
    "    print(\"Updating correspondence set...\")\n",
    "    correspondence_set = np.concatenate((correspondence_set, new_keypoints), axis=0)\n",
    "    print(\"Iteration completed in %0.4f seconds\" % (time.time() - t1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
